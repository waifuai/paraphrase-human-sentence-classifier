# Human vs. Machine Sentence Classifier (Hugging Face Transformers)

This project classifies whether a sentence is human-generated or machine-generated (e.g., paraphrased) using Hugging Face Transformers.

## Overview

The classifier utilizes a pre-trained Transformer model (e.g., DistilBERT, BERT) fine-tuned for sequence classification. It leverages the `transformers`, `datasets`, and `torch` libraries for efficient training and evaluation. The project structure is organized for clarity and maintainability.

## Setup

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd paraphrase-human-sentence-classifier
```

### 2. Create Virtual Environment and Install Dependencies

This project uses `uv` for environment and package management. Ensure you have Python 3.8+ installed.

```bash
# Install uv globally if you haven't already (one-time setup)
# pip install uv
# or consult uv documentation for preferred installation method

# Create the virtual environment
python -m uv venv .venv

# Ensure pip is available in the venv (recommended for compatibility)
.venv/Scripts/python.exe -m ensurepip

# Install uv within the venv (recommended)
.venv/Scripts/python.exe -m pip install uv

# Install project dependencies
.venv/Scripts/python.exe -m uv pip install -r requirements.txt

# To run tests later, install pytest
# .venv/Scripts/python.exe -m uv pip install pytest
```
*(Note: Use `.venv/bin/python` or `source .venv/bin/activate` on Linux/macOS instead of `.venv/Scripts/python.exe`)*

## Data Preparation

Prepare your training and evaluation data in **tab-separated value (TSV)** format with two columns: `text` and `label`.

-   **Column 1 (text):** The sentence (human or machine-generated).
-   **Column 2 (label):** The corresponding label (`1` for human, `0` for machine).

**Example (`data/train.tsv`):**

```tsv
This is a human sentence.	1
This text was generated by a machine.	0
Another example written by a person.	1
Paraphrased content often lacks nuance.	0
```

Place your training file (e.g., `train.tsv`) and evaluation file (e.g., `eval.tsv`) inside the `data/` directory, or specify their paths using command-line arguments when running the training script.

## Training the Model

Run the training script `src/scripts/train.py` using the Python interpreter from your virtual environment. Configuration is handled via command-line arguments.

**Basic Usage (using defaults):**

```bash
.venv/Scripts/python.exe -m scripts.train
```
*(or `python -m scripts.train` if `.venv` is activated and `src` is in PYTHONPATH)*

This will:
- Use `data/train.tsv` and `data/eval.tsv`.
- Use `distilbert-base-uncased` as the base model.
- Train for 3 epochs.
- Save the best model and logs to `model_output_hf/`.

**Custom Configuration:**

```bash
.venv/Scripts/python.exe -m scripts.train \
    --train_file path/to/your/train_data.tsv \
    --eval_file path/to/your/eval_data.tsv \
    --model_name_or_path bert-base-uncased \
    --output_dir custom_model_output/ \
    --num_train_epochs 5 \
    --learning_rate 2e-5 \
    --per_device_train_batch_size 16
```

Run ` .venv/Scripts/python.exe -m scripts.train --help` for a full list of options.

The script performs the following steps:
1.  Parses command-line arguments.
2.  Loads and tokenizes the datasets using `datasets` and `transformers.AutoTokenizer`.
3.  Loads the pre-trained model using `transformers.AutoModelForSequenceClassification`.
4.  Defines `TrainingArguments`.
5.  Initializes the `Trainer`.
6.  Trains the model, evaluating periodically and saving the best checkpoint based on F1-score.
7.  Saves the final best model, tokenizer, configuration, and training/evaluation metrics to the output directory.

## Evaluation

To evaluate a previously trained model saved by the training script, run `src/scripts/evaluate.py`.

**Usage:**

```bash
.venv/Scripts/python.exe -m scripts.evaluate \
    --model_dir path/to/your/saved_model_output \
    --eval_file path/to/your/eval_data.tsv
```

The script will:
1.  Load the model and tokenizer from `--model_dir`.
2.  Load and tokenize the evaluation data from `--eval_file`.
3.  Run predictions using the `Trainer`.
4.  Compute and print metrics: Accuracy, Precision, Recall, F1-score, and Confusion Matrix.
5.  Save the evaluation metrics to `evaluation_results.json` within the `--model_dir`.

## Project Structure

-   `src/classifier/data.py`: Handles data loading and tokenization using `datasets` and `transformers`.
-   `src/classifier/model.py`: Loads pre-trained sequence classification models from `transformers`.
-   `src/scripts/train.py`: Main script to run the training process using `transformers.Trainer`.
-   `src/scripts/evaluate.py`: Script to evaluate a saved model.
-   `data/`: Default directory for `train.tsv` and `eval.tsv` data files.
-   `model_output_hf/`: Default directory for saved models, checkpoints, and logs.
-   `tests/`: Contains unit tests (`pytest`).
-   `requirements.txt`: Lists Python dependencies (install via `uv pip install -r requirements.txt`).
-   `pytest.ini`: Configures `pytest` (e.g., sets PYTHONPATH).
-   `.gitignore`: Specifies intentionally untracked files for Git.
-   `README.md`: This file.

## Notes

-   Ensure the labels in your TSV files are `1` (human) and `0` (machine).
-   The default model is `distilbert-base-uncased`. You can specify others like `bert-base-uncased` via `--model_name_or_path`. Larger models require more memory/compute.
-   Training uses GPU if available and `torch` detects it, unless `--force_cpu` is specified.

## Future Improvements

-   **Prediction Script:** Create a script to load a trained model and classify new, unseen sentences interactively or from a file.
-   **Hyperparameter Tuning:** Integrate with libraries like Optuna or Ray Tune for systematic hyperparameter searches.
-   **Cross-Validation:** Implement k-fold cross-validation for more robust evaluation.
-   **Advanced Models:** Experiment with different Transformer architectures or custom heads.
-   **Packaging:** Add `pyproject.toml` if proper packaging for distribution (e.g., via PyPI) is desired.